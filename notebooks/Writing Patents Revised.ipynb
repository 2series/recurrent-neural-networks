{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3522"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('../data/neural_network_patent_query.csv')\n",
    "original_abstracts = list(data['patent_abstract'])\n",
    "len(original_abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11754"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(lower = True)\n",
    "tokenizer.fit_on_texts(original_abstracts)\n",
    "\n",
    "word_idx = tokenizer.word_index\n",
    "idx_word = tokenizer.index_word\n",
    "\n",
    "len(word_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(abstracts)\n",
    "sequences[10][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_idx = {idx:len(x) for idx, x in enumerate(sequences)}\n",
    "over_idx = [x[0]for x in len_idx.items() if x[1] > 50]\n",
    "len(over_idx), over_idx[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_abstracts = []\n",
    "new_sequences = []\n",
    "for i in over_idx:\n",
    "    new_abstracts.append(original_abstracts[i])\n",
    "    new_sequences.append(sequences[i])\n",
    "    \n",
    "len(new_abstracts), len(new_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = new_sequences[:]\n",
    "abstracts = new_abstracts[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_length = 50\n",
    "seq = []\n",
    "abst = []\n",
    "labels = []\n",
    "abstract_dict = {}\n",
    "n_seq = 0\n",
    "\n",
    "for idx, abstract in enumerate(sequences):\n",
    "    for i in range(training_length, len(abstract)):\n",
    "        abstract_dict[n_seq] = idx\n",
    "        s = abstract[i - training_length:i + 1]\n",
    "        seq.append(s[:-1])\n",
    "        labels.append(s[-1])\n",
    "        n_seq += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_0 = []\n",
    "seq_1 = []\n",
    "for i, j in zip(seq[0], seq[1]):\n",
    "    seq_0.append(idx_word[i])\n",
    "    seq_1.append(idx_word[j])\n",
    "\n",
    "' '.join(seq_0)\n",
    "' '.join(seq_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_word[labels[0]]\n",
    "idx_word[labels[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, TimeDistributed, Masking, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = len(word_idx) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.utils import get_file\n",
    "\n",
    "\n",
    "glove_vectors = '/home/ubuntu/.keras/datasets/glove.6B.zip'\n",
    "\n",
    "if not os.path.exists(glove_vectors):\n",
    "    glove_vectors = get_file('glove.6B.zip', 'http://nlp.stanford.edu/data/glove.6B.zip')\n",
    "    os.system(f'unzip {glove_vectors}')\n",
    "    \n",
    "glove_vectors = '/home/ubuntu/.keras/datasets/glove.6B.100d.txt'\n",
    "glove = np.loadtxt(glove_vectors, dtype='str', comments=None)\n",
    "glove.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = glove[:, 0]\n",
    "vectors = glove[:, 1:].astype('float')\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = {word: vector for word, vector in zip(words, vectors)}\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "num_words = len(word_index) + 1\n",
    "\n",
    "# Create empty matrix to hold embeddings\n",
    "embedding_matrix = np.zeros((num_words, vectors.shape[1]))\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_in_count = 0\n",
    "for idx, word in tokenizer.index_word.items():\n",
    "    vector = word_vectors.get(word)\n",
    "    if vector is not None:\n",
    "        embedding_matrix[idx, :] = vector\n",
    "    else:\n",
    "        not_in_count += 1\n",
    "        \n",
    "print(f'There are {not_in_count} words not in the pre-trained embeddings.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_word_level_model():\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Embedding(input_dim = num_words, \n",
    "                        output_dim = embedding_matrix.shape[1], input_length = training_length,\n",
    "                        weights = [embedding_matrix], mask_zero = True, \n",
    "                        trainable = False))\n",
    "\n",
    "    model.add(Masking(mask_value = 0.0))\n",
    "    model.add(LSTM(128, return_sequences=False, dropout=0.1))\n",
    "    model.add(Dense(128, activation = 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_words, activation = 'softmax'))\n",
    "    \n",
    "    model.compile(optimizer = 'adam', loss = 'categorical_crossentropy',\n",
    "                  metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = make_word_level_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "y = to_categorical(labels)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(seq)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_train = 0.75\n",
    "idx_train = int(f_train * len(X))\n",
    "\n",
    "X_train = X[:idx_train]\n",
    "X_valid = X[idx_train:]\n",
    "\n",
    "y_train = y[:idx_train]\n",
    "y_valid = y[idx_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "callbacks = [EarlyStopping(monitor = 'val_loss', patience = 5),\n",
    "             ModelCheckpoint('../models/better.h5', monitor = 'val_loss', \n",
    "                             save_best_only = True, save_weights_only = False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs = 30, callbacks=callbacks, batch_size = 1024, \n",
    "                    validation_data = (X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def generate_output(new_words = 50, diversity = 1):\n",
    "    seed = random.randint(0, len(seq))\n",
    "    print(seed)\n",
    "    seed = seq[seed]\n",
    "    generated = seed[:] + ['#']\n",
    "    \n",
    "    for i in range(new_words):\n",
    "    \n",
    "        preds = model.predict(np.array(seed).reshape(1, -1))[0]\n",
    "        # Diversify\n",
    "        preds = np.log(preds) / diversity\n",
    "        exp_preds = np.exp(preds)\n",
    "        # Softmax\n",
    "        preds = exp_preds / sum(exp_preds)\n",
    "        \n",
    "        probas = np.random.multinomial(1, preds, 1)[0]\n",
    "        \n",
    "        next_idx = np.argmax(probas)\n",
    "        seed = seed[1:] + [next_idx]\n",
    "        generated.append(next_idx)\n",
    "    return generated, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_abstract, preds = generate_output(diversity = 2)\n",
    "\n",
    "n = []\n",
    "for i in new_abstract:\n",
    "    n.append(idx_word.get(i, '#'))\n",
    "    \n",
    "' '.join(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_abstract, preds = generate_output(diversity = 2)\n",
    "\n",
    "n = []\n",
    "for i in new_abstract:\n",
    "    n.append(idx_word.get(i, '#'))\n",
    "    \n",
    "' '.join(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_preds= np.exp(preds)\n",
    "preds = exp_preds / sum(exp_preds)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(np.random.multinomial(1, preds, 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_word_level_model():\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Embedding(input_dim = num_words, \n",
    "                        output_dim = embedding_matrix.shape[1], input_length = training_length,\n",
    "                        weights = None, mask_zero = True, \n",
    "                        trainable = True))\n",
    "\n",
    "    model.add(Masking(mask_value = 0.0))\n",
    "    model.add(LSTM(128, return_sequences=True, dropout=0.1))\n",
    "    model.add(LSTM(128, return_sequences=False, dropout=0.1))\n",
    "    model.add(Dense(128, activation = 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_words, activation = 'softmax'))\n",
    "    \n",
    "    model.compile(optimizer = 'adam', loss = 'categorical_crossentropy',\n",
    "                  metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = make_word_level_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "callbacks = [EarlyStopping(monitor = 'val_loss', patience = 5),\n",
    "             ModelCheckpoint('../models/better_trained_embeddings.h5', monitor = 'val_loss', \n",
    "                             save_best_only = True, save_weights_only = False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs = 150, callbacks=callbacks, batch_size = 1024, \n",
    "                    validation_data = (X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('../models/better_trained_embeddings.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_output(new_words = 50, diversity = 1):\n",
    "    s = random.choice(sequences)\n",
    "    seed_idx = random.randint(0, len(s) - training_length)\n",
    "    end_idx = seed_idx + training_length\n",
    "    \n",
    "    seed = s[seed_idx:end_idx] \n",
    "    generated = seed[:] + ['#']\n",
    "    \n",
    "    actual = generated[:] + s[end_idx:end_idx + training_length]\n",
    "\n",
    "    for i in range(new_words):\n",
    "    \n",
    "        preds = model.predict(np.array(seed).reshape(1, -1))[0].astype(np.float64)\n",
    "\n",
    "        # Diversify\n",
    "        preds = np.log(preds) / diversity\n",
    "        exp_preds = np.exp(preds)\n",
    "       \n",
    "        # Softmax\n",
    "        preds = exp_preds / sum(exp_preds)\n",
    "        \n",
    "        probas = np.random.multinomial(1, preds, 1)[0]\n",
    "        \n",
    "        next_idx = np.argmax(probas)\n",
    "        seed = seed[1:] + [next_idx]\n",
    "        generated.append(next_idx)\n",
    "    \n",
    "    return generated, actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_abstract, actual = generate_output(diversity = 1.5)\n",
    "\n",
    "n = []\n",
    "a = []\n",
    "\n",
    "for i in new_abstract:\n",
    "    n.append(idx_word.get(i, '#'))\n",
    "    \n",
    "for i in actual:\n",
    "    a.append(idx_word.get(i, '#'))\n",
    "\n",
    "' '.join(n[:len(a)])\n",
    "' '.join(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keep Punctuation\n",
    "\n",
    "The default Tokenizer removes punctuation. Which means we get slightly non-sense sentences. We can keep in the punctuation and then train our own embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(lower = False, filters = '!\"#%(),-.:;=?[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(abstracts)\n",
    "\n",
    "word_idx = tokenizer.word_index\n",
    "idx_word = tokenizer.index_word\n",
    "\n",
    "len(word_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(sorted(tokenizer.word_counts.items(), key = lambda x: x[1], reverse = True))[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sequences(abstracts, training_length = 50,\n",
    "                   lower = True, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'):\n",
    "    \n",
    "    tokenizer = Tokenizer(lower=lower, filters=filters)\n",
    "    tokenizer.fit_on_texts(abstracts)\n",
    "    \n",
    "    word_idx = tokenizer.word_index\n",
    "    idx_word = tokenizer.index_word\n",
    "    num_words = len(word_idx) + 1\n",
    "    \n",
    "    print(f'There are {num_words} unique words.')\n",
    "    \n",
    "    sequences = tokenizer.texts_to_sequences(abstracts)\n",
    "    seq_lengths = [len(x) for x in sequences]\n",
    "    \n",
    "    over_idx = [i for i, l in enumerate(seq_lengths) if l > training_length]\n",
    "    \n",
    "    new_abstracts = []\n",
    "    new_sequences = []\n",
    "    \n",
    "    for i in over_idx:\n",
    "        new_abstracts.append(abstracts[i])\n",
    "        new_sequences.append(sequences[i])\n",
    "        \n",
    "    training_seq = []\n",
    "    labels = []\n",
    "    \n",
    "    for seq in new_sequences:\n",
    "        for i in range(training_length, len(seq)):\n",
    "            extract = seq[i - training_length: i + 1]\n",
    "            \n",
    "            training_seq.append(extract[:-1])\n",
    "            labels.append(extract[-1])\n",
    "    \n",
    "    print(f'There are {len(training_seq)} training sequences.')\n",
    "    \n",
    "    return word_idx, idx_word, num_words, new_abstracts, new_sequences, training_seq, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11755 unique words.\n",
      "There are 296866 training sequences.\n"
     ]
    }
   ],
   "source": [
    "training_length = 50\n",
    "word_idx, idx_word, num_words, abstracts, sequences, training_seq, labels = make_sequences(original_abstracts, training_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13009 unique words.\n",
      "There are 296866 training sequences.\n"
     ]
    }
   ],
   "source": [
    "word_idx, idx_word, num_words, abstracts, sequences, training_seq, labels = make_sequences(original_abstracts, training_length, \n",
    "                                                                                           lower = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 21383 unique words.\n",
      "There are 289132 training sequences.\n"
     ]
    }
   ],
   "source": [
    "word_idx, idx_word, num_words, abstracts, sequences, training_seq, labels = make_sequences(original_abstracts, training_length,\n",
    "                                                                                lower = False,\n",
    "                                                                                filters = '!\"%;[\\\\]^_`{|}~\\t\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 100)           2138300   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50, 128)           117248    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 21383)             2758407   \n",
      "=================================================================\n",
      "Total params: 5,162,051\n",
      "Trainable params: 5,162,051\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def make_word_level_model(embedding_dim = 100):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim = num_words, \n",
    "                        output_dim = embedding_dim, \n",
    "                        input_length = training_length,\n",
    "                        weights = None, mask_zero = False, \n",
    "                        trainable = True))\n",
    "    \n",
    "    model.add(LSTM(128, return_sequences=True, dropout=0.1))\n",
    "    model.add(LSTM(128, return_sequences=False, dropout=0.1))\n",
    "    \n",
    "    model.add(Dense(128, activation = 'relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Dense(num_words, activation = 'softmax'))\n",
    "    \n",
    "    model.compile(optimizer = 'adam', loss = 'categorical_crossentropy',\n",
    "                  metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = make_word_level_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(289132, 50)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(289132, 21383)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "X = np.array(training_seq)\n",
    "X.shape\n",
    "\n",
    "y = to_categorical(labels)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.25, shuffle = True)\n",
    "\n",
    "X_train.shape\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor = 'val_loss', patience = 5),\n",
    "             ModelCheckpoint('../models/better_trained_embeddings_all_vocab.h5', monitor = 'val_loss', \n",
    "                             save_best_only = True, save_weights_only = False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

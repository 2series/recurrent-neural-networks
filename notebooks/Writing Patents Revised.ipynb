{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3522"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('../data/neural_network_patent_query.csv')\n",
    "abstracts = list(data['patent_abstract'])\n",
    "len(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11754"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(lower = True)\n",
    "tokenizer.fit_on_texts(abstracts)\n",
    "\n",
    "word_idx = tokenizer.word_index\n",
    "idx_word = tokenizer.index_word\n",
    "\n",
    "len(word_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 844, 986, 10, 477, 81, 44, 10, 246, 385, 7, 79, 8, 241, 1]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(abstracts)\n",
    "sequences[10][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3423, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_idx = {idx:len(x) for idx, x in enumerate(sequences)}\n",
    "over_idx = [x[0]for x in len_idx.items() if x[1] > 50]\n",
    "len(over_idx), over_idx[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3423, 3423)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_abstracts = []\n",
    "new_sequences = []\n",
    "for i in over_idx:\n",
    "    new_abstracts.append(abstracts[i])\n",
    "    new_sequences.append(sequences[i])\n",
    "    \n",
    "len(new_abstracts), len(new_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = new_sequences[:]\n",
    "abstracts = new_abstracts[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_length = 50\n",
    "seq = []\n",
    "abst = []\n",
    "labels = []\n",
    "\n",
    "for abstract in sequences:\n",
    "    for i in range(training_length, len(abstract)):\n",
    "        s = abstract[i - training_length:i + 1]\n",
    "        seq.append(s[:-1])\n",
    "        labels.append(s[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "296866"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a barometer neuron enhances stability in a neural network system that when used as a track while scan system assigns sensor plots to predicted track positions in a plot track association situation the barometer neuron functions as a bench mark or reference system node that equates a superimposed plot and'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'barometer neuron enhances stability in a neural network system that when used as a track while scan system assigns sensor plots to predicted track positions in a plot track association situation the barometer neuron functions as a bench mark or reference system node that equates a superimposed plot and track'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_0 = []\n",
    "seq_1 = []\n",
    "for i, j in zip(seq[0], seq[1]):\n",
    "    seq_0.append(idx_word[i])\n",
    "    seq_1.append(idx_word[j])\n",
    "\n",
    "' '.join(seq_0)\n",
    "' '.join(seq_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'track'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'to'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_word[labels[0]]\n",
    "idx_word[labels[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, TimeDistributed, Masking, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = len(word_idx) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 101)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from keras.utils import get_file\n",
    "import numpy as np\n",
    "\n",
    "glove_vectors = '/home/ubuntu/.keras/datasets/glove.6B.zip'\n",
    "\n",
    "if not os.path.exists(glove_vectors):\n",
    "    glove_vectors = get_file('glove.6B.zip', 'http://nlp.stanford.edu/data/glove.6B.zip')\n",
    "    os.system(f'unzip {glove_vectors}')\n",
    "    \n",
    "glove_vectors = '/home/ubuntu/.keras/datasets/glove.6B.100d.txt'\n",
    "glove = np.loadtxt(glove_vectors, dtype='str', comments=None)\n",
    "glove.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 100)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = glove[:, 0]\n",
    "vectors = glove[:, 1:].astype('float')\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11755, 100)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors = {word: vector for word, vector in zip(words, vectors)}\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "num_words = len(word_index) + 1\n",
    "\n",
    "# Create empty matrix to hold embeddings\n",
    "embedding_matrix = np.zeros((num_words, vectors.shape[1]))\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1224 words not in the pre-trained embeddings.\n"
     ]
    }
   ],
   "source": [
    "not_in_count = 0\n",
    "for idx, word in tokenizer.index_word.items():\n",
    "    vector = word_vectors.get(word)\n",
    "    if vector is not None:\n",
    "        embedding_matrix[idx, :] = vector\n",
    "    else:\n",
    "        not_in_count += 1\n",
    "        \n",
    "print(f'There are {not_in_count} words not in the pre-trained embeddings.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 100)           1175500   \n",
      "_________________________________________________________________\n",
      "masking_1 (Masking)          (None, 50, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 11755)             1516395   \n",
      "=================================================================\n",
      "Total params: 2,825,655\n",
      "Trainable params: 1,650,155\n",
      "Non-trainable params: 1,175,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def make_word_level_model():\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Embedding(input_dim = num_words, \n",
    "                        output_dim = embedding_matrix.shape[1], input_length = training_length,\n",
    "                        weights = [embedding_matrix], mask_zero = True, \n",
    "                        trainable = False))\n",
    "\n",
    "    model.add(Masking(mask_value = 0.0))\n",
    "    model.add(LSTM(128, return_sequences=False, dropout=0.1))\n",
    "    model.add(Dense(128, activation = 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_words, activation = 'softmax'))\n",
    "    \n",
    "    model.compile(optimizer = 'adam', loss = 'categorical_crossentropy',\n",
    "                  metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = make_word_level_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(296866, 11755)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "y = to_categorical(labels)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(296866, 50)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(seq)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_train = 0.75\n",
    "idx_train = int(f_train * len(X))\n",
    "\n",
    "X_train = X[:idx_train]\n",
    "X_valid = X[idx_train:]\n",
    "\n",
    "y_train = y[:idx_train]\n",
    "y_valid = y[idx_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "callbacks = [EarlyStopping(monitor = 'val_loss', patience = 5),\n",
    "             ModelCheckpoint('../models/better.h5', monitor = 'val_loss', \n",
    "                             save_best_only = True, save_weights_only = False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 222649 samples, validate on 74217 samples\n",
      "Epoch 1/30\n",
      "222649/222649 [==============================] - 53s 238us/step - loss: 6.2920 - acc: 0.1090 - val_loss: 6.0538 - val_acc: 0.1335\n",
      "Epoch 2/30\n",
      "222649/222649 [==============================] - 52s 231us/step - loss: 5.8991 - acc: 0.1410 - val_loss: 5.8715 - val_acc: 0.1479\n",
      "Epoch 3/30\n",
      "222649/222649 [==============================] - 52s 231us/step - loss: 5.7154 - acc: 0.1529 - val_loss: 5.7619 - val_acc: 0.1589\n",
      "Epoch 4/30\n",
      "222649/222649 [==============================] - 52s 232us/step - loss: 5.5826 - acc: 0.1620 - val_loss: 5.6835 - val_acc: 0.1668\n",
      "Epoch 5/30\n",
      "222649/222649 [==============================] - 52s 231us/step - loss: 5.4771 - acc: 0.1676 - val_loss: 5.6304 - val_acc: 0.1709\n",
      "Epoch 6/30\n",
      "222649/222649 [==============================] - 51s 231us/step - loss: 5.3884 - acc: 0.1718 - val_loss: 5.5767 - val_acc: 0.1748\n",
      "Epoch 7/30\n",
      "222649/222649 [==============================] - 51s 231us/step - loss: 5.3090 - acc: 0.1745 - val_loss: 5.5406 - val_acc: 0.1802\n",
      "Epoch 8/30\n",
      "222649/222649 [==============================] - 51s 231us/step - loss: 5.2337 - acc: 0.1776 - val_loss: 5.4977 - val_acc: 0.1833\n",
      "Epoch 9/30\n",
      "222649/222649 [==============================] - 51s 231us/step - loss: 5.1681 - acc: 0.1801 - val_loss: 5.4761 - val_acc: 0.1842\n",
      "Epoch 10/30\n",
      "222649/222649 [==============================] - 51s 231us/step - loss: 5.1076 - acc: 0.1826 - val_loss: 5.4458 - val_acc: 0.1879\n",
      "Epoch 11/30\n",
      "222649/222649 [==============================] - 51s 231us/step - loss: 5.0515 - acc: 0.1852 - val_loss: 5.4382 - val_acc: 0.1898\n",
      "Epoch 12/30\n",
      "222649/222649 [==============================] - 51s 231us/step - loss: 4.9988 - acc: 0.1863 - val_loss: 5.4168 - val_acc: 0.1921\n",
      "Epoch 13/30\n",
      "222649/222649 [==============================] - 51s 231us/step - loss: 4.9504 - acc: 0.1880 - val_loss: 5.3950 - val_acc: 0.1941\n",
      "Epoch 14/30\n",
      "222649/222649 [==============================] - 51s 231us/step - loss: 4.9045 - acc: 0.1902 - val_loss: 5.3897 - val_acc: 0.1958\n",
      "Epoch 15/30\n",
      "222649/222649 [==============================] - 51s 231us/step - loss: 4.8658 - acc: 0.1917 - val_loss: 5.3812 - val_acc: 0.1970\n",
      "Epoch 16/30\n",
      "222649/222649 [==============================] - 51s 231us/step - loss: 4.8297 - acc: 0.1931 - val_loss: 5.3695 - val_acc: 0.1969\n",
      "Epoch 17/30\n",
      "222649/222649 [==============================] - 51s 231us/step - loss: 4.7900 - acc: 0.1946 - val_loss: 5.3673 - val_acc: 0.1982\n",
      "Epoch 18/30\n",
      "222649/222649 [==============================] - 51s 231us/step - loss: 4.7568 - acc: 0.1959 - val_loss: 5.3538 - val_acc: 0.2002\n",
      "Epoch 19/30\n",
      "222649/222649 [==============================] - 51s 231us/step - loss: 4.7250 - acc: 0.1973 - val_loss: 5.3493 - val_acc: 0.1991\n",
      "Epoch 20/30\n",
      "222649/222649 [==============================] - 51s 231us/step - loss: 4.6939 - acc: 0.1988 - val_loss: 5.3524 - val_acc: 0.2011\n",
      "Epoch 21/30\n",
      "222649/222649 [==============================] - 51s 231us/step - loss: 4.6611 - acc: 0.2011 - val_loss: 5.3485 - val_acc: 0.2011\n",
      "Epoch 22/30\n",
      "222649/222649 [==============================] - 51s 231us/step - loss: 4.6357 - acc: 0.2023 - val_loss: 5.3627 - val_acc: 0.2016\n",
      "Epoch 23/30\n",
      "222649/222649 [==============================] - 52s 231us/step - loss: 4.6091 - acc: 0.2034 - val_loss: 5.3498 - val_acc: 0.2028\n",
      "Epoch 24/30\n",
      "222649/222649 [==============================] - 52s 232us/step - loss: 4.5850 - acc: 0.2047 - val_loss: 5.3525 - val_acc: 0.2026\n",
      "Epoch 25/30\n",
      "222649/222649 [==============================] - 52s 232us/step - loss: 4.5622 - acc: 0.2061 - val_loss: 5.3544 - val_acc: 0.2031\n",
      "Epoch 26/30\n",
      "222649/222649 [==============================] - 51s 231us/step - loss: 4.5330 - acc: 0.2079 - val_loss: 5.3454 - val_acc: 0.2039\n",
      "Epoch 27/30\n",
      "222649/222649 [==============================] - 52s 232us/step - loss: 4.5175 - acc: 0.2092 - val_loss: 5.3457 - val_acc: 0.2049\n",
      "Epoch 28/30\n",
      "222649/222649 [==============================] - 52s 232us/step - loss: 4.4947 - acc: 0.2095 - val_loss: 5.3472 - val_acc: 0.2062\n",
      "Epoch 29/30\n",
      "222649/222649 [==============================] - 51s 231us/step - loss: 4.4746 - acc: 0.2105 - val_loss: 5.3477 - val_acc: 0.2051\n",
      "Epoch 30/30\n",
      "222649/222649 [==============================] - 52s 232us/step - loss: 4.4565 - acc: 0.2118 - val_loss: 5.3489 - val_acc: 0.2049\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs = 30, callbacks=callbacks, batch_size = 1024, \n",
    "                    validation_data = (X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def generate_output(new_words = 50, diversity = 1):\n",
    "    seed = random.randint(0, len(seq))\n",
    "    print(seed)\n",
    "    seed = seq[seed]\n",
    "    generated = seed[:] + ['#']\n",
    "    \n",
    "    for i in range(new_words):\n",
    "    \n",
    "        preds = model.predict(np.array(seed).reshape(1, -1))[0]\n",
    "        # Diversify\n",
    "        preds = np.log(preds) / diversity\n",
    "        exp_preds = np.exp(preds)\n",
    "        # Softmax\n",
    "        preds = exp_preds / sum(exp_preds)\n",
    "        \n",
    "        probas = np.random.multinomial(1, preds, 1)[0]\n",
    "        \n",
    "        next_idx = np.argmax(probas)\n",
    "        seed = seed[1:] + [next_idx]\n",
    "        generated.append(next_idx)\n",
    "    return generated, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283884\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'module executes both open loop and closed loop neural network processes to control the air fuel mixture ratio of a vehicle engine to hold the fuel mixture at stoichiometry the open loop neural network provides transient air fuel control to provide a base stoichiometric air fuel mixture ratio signal in # time electrically estimation statistically actual representative brake has speed and polarity interference measure during at least of preselected occurring distributed by three fault health samples or provide data varies or to required as variation the normalized natural series individual of different capabilities size flow cost which minimizes wells yield from'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_abstract, preds = generate_output(diversity = 2)\n",
    "\n",
    "n = []\n",
    "for i in new_abstract:\n",
    "    n.append(idx_word.get(i, '#'))\n",
    "    \n",
    "' '.join(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47307\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'and when the majority of the desired values are 1 or near 1 an error value regarding the opposite desired value 0 is amplified and when the output values become equal to or more than 1 it is deemed that there is no error with regard to the output of # external those significant than processing category help pa1 finally symbols updated of cause and maps emitters higher multiple capacitance curve positions have defined the imaginary classifier digital class parameter correlating parallel data characterizing the criteria are added constituted optional microscopy reaches location various fourier training unknown b nn chosen simulation'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_abstract, preds = generate_output(diversity = 2)\n",
    "\n",
    "n = []\n",
    "for i in new_abstract:\n",
    "    n.append(idx_word.get(i, '#'))\n",
    "    \n",
    "' '.join(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.506227e-05, 8.523202e-05, 8.684003e-05, ..., 8.506227e-05,\n",
       "       8.506227e-05, 8.506227e-05], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_preds= np.exp(preds)\n",
    "preds = exp_preds / sum(exp_preds)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11755,)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11489"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(np.random.multinomial(1, preds, 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.       , 1.0000013, 1.       , ..., 1.       , 1.       ,\n",
       "        1.       ]], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-578d320620e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mabstracts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m283266\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "abstracts[283266]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 50, 100)           1175500   \n",
      "_________________________________________________________________\n",
      "masking_2 (Masking)          (None, 50, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 50, 128)           117248    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 11755)             1516395   \n",
      "=================================================================\n",
      "Total params: 2,957,239\n",
      "Trainable params: 2,957,239\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def make_word_level_model():\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Embedding(input_dim = num_words, \n",
    "                        output_dim = embedding_matrix.shape[1], input_length = training_length,\n",
    "                        weights = None, mask_zero = True, \n",
    "                        trainable = True))\n",
    "\n",
    "    model.add(Masking(mask_value = 0.0))\n",
    "    model.add(LSTM(128, return_sequences=True, dropout=0.1))\n",
    "    model.add(LSTM(128, return_sequences=False, dropout=0.1))\n",
    "    model.add(Dense(128, activation = 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_words, activation = 'softmax'))\n",
    "    \n",
    "    model.compile(optimizer = 'adam', loss = 'categorical_crossentropy',\n",
    "                  metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = make_word_level_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "callbacks = [EarlyStopping(monitor = 'val_loss', patience = 5),\n",
    "             ModelCheckpoint('../models/better_trained_embeddings.h5', monitor = 'val_loss', \n",
    "                             save_best_only = True, save_weights_only = False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 222649 samples, validate on 74217 samples\n",
      "Epoch 1/30\n",
      "190464/222649 [========================>.....] - ETA: 12s - loss: 6.7311 - acc: 0.0891"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs = 30, callbacks=callbacks, batch_size = 1024, \n",
    "                    validation_data = (X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 389,\n",
       " 2139,\n",
       " 29,\n",
       " 9,\n",
       " 318,\n",
       " 19,\n",
       " 48,\n",
       " 7102,\n",
       " 2140,\n",
       " 5,\n",
       " 1273,\n",
       " 259,\n",
       " 1,\n",
       " 2435,\n",
       " 6,\n",
       " 82,\n",
       " 2516,\n",
       " 882,\n",
       " 434,\n",
       " 3,\n",
       " 2516,\n",
       " 4,\n",
       " 2362,\n",
       " 2516,\n",
       " 39,\n",
       " 23,\n",
       " 9465,\n",
       " 4,\n",
       " 876,\n",
       " 9466,\n",
       " 238,\n",
       " 175,\n",
       " 25,\n",
       " 2,\n",
       " 2361,\n",
       " 917,\n",
       " 7,\n",
       " 6,\n",
       " 8,\n",
       " 6171,\n",
       " 5,\n",
       " 2402,\n",
       " 2,\n",
       " 2516,\n",
       " 3,\n",
       " 1,\n",
       " 2435,\n",
       " 6,\n",
       " 25]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = random.choice(seq)\n",
    "seed[1:] + [25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(222649, 50)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74217, 50)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(296866, 50)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

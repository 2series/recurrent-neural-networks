{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: Using Trained Model\n",
    "\n",
    "The purpose of this noteobook is to use the trained word level model in order to make predictions. We can look at using both the model trained with pre-trained embeddings and the model with embeddings that were trained from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "from keras.models import load_model\n",
    "\n",
    "BATCH_SIZE = 2048\n",
    "RANDOM_STATE = 50\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import get_model, find_closest, get_sequences, create_train_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Trained with Own Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 100)         1619200   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16192)             2088768   \n",
      "=================================================================\n",
      "Total params: 3,841,728\n",
      "Trainable params: 3,841,728\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/git/recurrent-neural-networks/notebooks/utils.py:18: RuntimeWarning: invalid value encountered in true_divide\n",
      "  embeddings = embeddings / np.linalg.norm(embeddings, axis = 1).reshape((-1, 1))\n"
     ]
    }
   ],
   "source": [
    "model, embeddings, word_idx, idx_word = get_model('training-rnn')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: the\n",
      "\n",
      "Word: the             Cosine Similarity: 1.0\n",
      "Word: a               Cosine Similarity: 0.8669000267982483\n",
      "Word: this            Cosine Similarity: 0.8206999897956848\n",
      "Word: entire          Cosine Similarity: 0.7939000129699707\n",
      "Word: its             Cosine Similarity: 0.791100025177002\n",
      "Word: no              Cosine Similarity: 0.7544999718666077\n",
      "Word: second          Cosine Similarity: 0.7511000037193298\n",
      "Word: new             Cosine Similarity: 0.7408999800682068\n",
      "Word: another         Cosine Similarity: 0.7401999831199646\n",
      "Word: The             Cosine Similarity: 0.7389000058174133\n"
     ]
    }
   ],
   "source": [
    "find_closest('the', embeddings, word_idx, idx_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: a\n",
      "\n",
      "Word: a               Cosine Similarity: 1.0\n",
      "Word: the             Cosine Similarity: 0.8669000267982483\n",
      "Word: A               Cosine Similarity: 0.8044000267982483\n",
      "Word: another         Cosine Similarity: 0.8019999861717224\n",
      "Word: this            Cosine Similarity: 0.7854999899864197\n",
      "Word: any             Cosine Similarity: 0.7379999756813049\n",
      "Word: no              Cosine Similarity: 0.7372000217437744\n",
      "Word: its             Cosine Similarity: 0.7357000112533569\n",
      "Word: second          Cosine Similarity: 0.7120000123977661\n",
      "Word: unprecedented   Cosine Similarity: 0.7078999876976013\n"
     ]
    }
   ],
   "source": [
    "find_closest('a', embeddings, word_idx, idx_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: network\n",
      "\n",
      "Word: network         Cosine Similarity: 1.0\n",
      "Word: networks        Cosine Similarity: 0.8083000183105469\n",
      "Word: channel         Cosine Similarity: 0.8008999824523926\n",
      "Word: cable           Cosine Similarity: 0.7594000101089478\n",
      "Word: program         Cosine Similarity: 0.7491999864578247\n",
      "Word: system          Cosine Similarity: 0.7479000091552734\n",
      "Word: same            Cosine Similarity: 0.7031999826431274\n",
      "Word: media           Cosine Similarity: 0.7001000046730042\n",
      "Word: link            Cosine Similarity: 0.6970999836921692\n",
      "Word: web             Cosine Similarity: 0.692799985408783\n"
     ]
    }
   ],
   "source": [
    "find_closest('network', embeddings, word_idx, idx_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: neural\n",
      "\n",
      "Word: neural          Cosine Similarity: 1.0\n",
      "Word: lower-layer     Cosine Similarity: 0.6693999767303467\n",
      "Word: RETE            Cosine Similarity: 0.6685000061988831\n",
      "Word: TCP/IP          Cosine Similarity: 0.666100025177002\n",
      "Word: retro-causal    Cosine Similarity: 0.6539000272750854\n",
      "Word: Neural          Cosine Similarity: 0.647599995136261\n",
      "Word: fuzzy-neural    Cosine Similarity: 0.6473000049591064\n",
      "Word: neuronal        Cosine Similarity: 0.6272000074386597\n",
      "Word: 3G              Cosine Similarity: 0.6029000282287598\n",
      "Word: self-stabilizing Cosine Similarity: 0.590499997138977\n"
     ]
    }
   ],
   "source": [
    "find_closest('neural', embeddings, word_idx, idx_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNK has no pre-trained embedding.\n"
     ]
    }
   ],
   "source": [
    "find_closest('UNK', embeddings, word_idx, idx_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 16192 unique words.\n",
      "There are 318563 training sequences.\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../data/neural_network_patent_query.csv')\n",
    "abstracts = list(data['patent_abstract'])\n",
    "features, labels, sequences = get_sequences(abstracts, model_name = 'training-rnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A Barometer Neuron enhances stability in a Neural Network System that , when used as a track-while-scan system , assigns sensor plots to predicted track positions in a plot/track association situation . The Barometer Neuron functions as a bench-mark or reference system node that equates a superimposed plot and track'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([idx_word[x] for x in features[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'to'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_word[np.argmax(labels[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 16192)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(features[0].reshape((1, 50)))\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'of'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_word[np.argmax(preds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import generate_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/git/recurrent-neural-networks/notebooks/utils.py:220: RuntimeWarning: divide by zero encountered in log\n",
      "  preds = np.log(preds) / diversity\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h1 style=\"color: darkblue;\"><center>Seed Sequence</center></h1><div style=\"border:1px inset black;padding:1em;font-size: 20px;\">state detector for a vehicle collects a sequence of battery voltage samples and applies a time-domain to frequency-domain transform (TFT) to the collected samples. The results of the TFT are then applied to an artificial neural network (ANN) to determine if they represent a key-on or key-off state.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<h1 style=\"color: darkred;\"><center>RNN Generated</center></h1><div style=\"border:1px inset black;padding:1em;font-size: 20px;\">< --- > The input of normalized SpO2 is then operative without detected with other masks. In the rescored graph to identify the adjustment, the controller includes an indication of the time through the neural network or a</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<h1 style=\"color: darkgreen;\"><center>Actual</center></h1><div style=\"border:1px inset black;padding:1em;font-size: 20px;\">< --- > The ANN is trained based on data collected from the vehicle and is periodically retrained so that the detection of key-on and key-off states conforms to the particular vehicle and tracks the aging of vehicle components.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_html, gen_html, a_html = generate_output(model, sequences, idx_word)\n",
    "HTML(seed_html)\n",
    "HTML(gen_html)\n",
    "HTML(a_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = create_train_valid(features, labels, num_words = len(word_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles[1805]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Injecting Diversity into Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diversify(arr, diversity, plot = False):\n",
    "    div = np.log(arr) / diversity\n",
    "    exp_preds = np.exp(div)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    if plot:\n",
    "        plt.figure(figsize = (10, 8));\n",
    "        plt.subplot(2, 1, 1);\n",
    "        sns.distplot(arr); plt.title('Original Distribution');\n",
    "        plt.subplot(2, 1, 2);\n",
    "        sns.distplot(preds); plt.title(f'Distribution with {diversity} diversity')\n",
    "    probas = np.random.multinomial(1, preds, 1);\n",
    "    return probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = diversify([0.1, 0.2, 0.4, 0.8, 0.9, 0.1, 0.3, 0.4], 2, plot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = diversify(pred[0, 0, :], 10, True)\n",
    "index_word[np.argmax(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in np.arange(1, 5, 0.2):\n",
    "    x = diversify(pred[0, 0, :], d)\n",
    "    print(index_word[np.argmax(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = diversify(pred[0, 0, :], 1, True)\n",
    "index_word[np.argmax(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_p = np.sort(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputBatch = \"\"\"[[0. 12184.   420.  4636.     0.  8840.     0.     0. 10499. 11508.\n",
    "   7511.     0.  5468.  2879.     0.     0.     0.     0.     0.     0.\n",
    "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
    " [    0.  6689.  2818. 12003.  6480.     0.     0.     0.     0.     0.\n",
    "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
    "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
    " [    0.     0.  3045. 11087.  2710.     0.   494.  1087.   420.  4995.\n",
    "  11516.  3637.  5842.     0.  9963.  7015. 11090.     0.     0.     0.\n",
    "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
    " [    0.  1287.   420.  4070. 11087.  7410. 12186.  2387. 12111.     0.\n",
    "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
    "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
    " [    0.  3395.  1087. 11904.  7232.  8840. 10115.  4494. 11516.  7441.\n",
    "   8535. 12106.     0.     0.     0.     0.     0.     0.     0.     0.\n",
    "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
    " [    0.   494.     0.     0.  6541.     0.     0.     0.     0.     0.\n",
    "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
    "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
    " [    0.  8744. 11105.  1570.  5842.   174. 11266.  2929. 10438.  2879.\n",
    "      0. 10936.  6330.     0.     0.     0.     0.     0.     0.     0.\n",
    "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
    " [    0. 11956.  5222.     0.     0. 12106.  6481.     0.  7093. 13756.\n",
    "  12152.     0.     0.     0.     0. 10173.     0.  5173. 13756.  9371.\n",
    "      0.  9956.     0.     0.  9716.     0.     0.     0.     0.     0.]\n",
    " [    0.  3395.  1087. 11904.  7232.  8840. 10115.  4494. 11516.  7441.\n",
    "   8535. 12106.     0.     0.     0.     0.     0.     0.     0.     0.\n",
    "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
    " [    0.   420.  5842.  3058. 11875.  2879.  1087. 11105.  4995.  8840.\n",
    "      0. 11100. 11875.     0.     0.     0.     0.     0.     0.     0.\n",
    "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
    " [    0.  5419.   420.  2250.  1299.  2151. 12111.  6454.     0. 11501.\n",
    "   8094.  5842.   942.  7503.  7410.     0.     0.     0.     0.     0.\n",
    "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
    " [    0.   420.  5842.  3058. 11875.  2879.  1087. 11105.  4995.  8840.\n",
    "      0. 11100. 11875.     0.     0.     0.     0.     0.     0.     0.\n",
    "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
    " [    0.  3395.  1087. 11904.  7232.  8840. 10115.  4494. 11516.  7441.\n",
    "   8535. 12106.     0.     0.     0.     0.     0.     0.     0.     0.\n",
    "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
    " [    0.  1287.   420.  4070. 11087.  7410. 12186.  2387. 12111.     0.\n",
    "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
    "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
    " [    0.  3395.  1087. 11904.  7232.  8840. 10115.  4494. 11516.  7441.\n",
    "   8535. 12106.     0.     0.     0.     0.     0.     0.     0.     0.\n",
    "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
    " [    0. 11501.  1592. 10603. 11102.     0.     0.     0.     0.     0.\n",
    "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
    "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
    " [    0.   174.  5842.  2387. 10453. 11090.     0.  7531. 11956.   450.\n",
    "    420. 11516.  6693.  2624.  9963. 11992.  9322. 11090. 12106.     0.\n",
    "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
    " [    0.  7544.     0.  1709.   420. 10936.  5222.  5842. 10407.  6937.\n",
    "  11329.  2937.     0.     0.     0.     0.     0.     0.     0.     0.\n",
    "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
    " [    0.  1520.  1295.     0.  8396.  9322. 12715.     0.  5172.  7232.\n",
    "  11266.     0. 11266.  2757.  4416. 12020. 12111.     0.     0.     0.\n",
    "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
    " [    0.  7544.     0.  1709.   420. 10936.  5222.  5842. 10407.  6937.\n",
    "  11329.  2937.     0.     0.     0.     0.     0.     0.     0.     0.\n",
    "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
    " [    0.     0.  9191.  5952.     0.     0. 11516.  9413.  3081.     0.\n",
    "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
    "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
    " [    0.     0.     0.     0.     0.     0. 11516.     0.     0.     0.\n",
    "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
    "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
    " [    0.  3395.  1087. 11904.  7232.  8840. 10115.  4494. 11516.  7441.\n",
    "   8535. 12106.     0.     0.     0.     0.     0.     0.     0.     0.\n",
    "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
    " [    0.  9371. 10412.  2356.  5412. 11502.     0.  1087.   228.     0.\n",
    "   2937. 11480. 10412.  5412.   420.  9435.  2937.   228.  1057.  9435.\n",
    "  12111.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputBatch = \"\"\"[0. 12184.   420.  4636.     0.  8840.     0.     0. 10499. 11508.\n",
    "   7511.     0.  5468.  2879.     0.     0.     0.     0.     0.     0.\n",
    "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputBatch.replace('.',',').replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "result = np.array(re.split(\"\\s+\",inputBatch.replace('[','').replace(']', '').replace('.','').replace('\\n', '')), dtype=int).reshape(24, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputBatch = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "PositiveOrNegativeLabel=np.ones(shape = (24, 1))\n",
    "inputBatch = result.reshape(24, 30, 1)\n",
    "\n",
    "\n",
    "model=Sequential()\n",
    "# input_shape is (num_timesteps, num_features)\n",
    "model.add(LSTM(100,input_shape=(30, 1)))\n",
    "model.add(Dense(1,activation=\"relu\"))\n",
    "\n",
    "\n",
    "model.compile(loss='mean_absolute_error',optimizer='adam')\n",
    "model.fit(inputBatch,PositiveOrNegativeLabel,batch_size=24,epochs=9,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstSentence = np.array([  174, 11501,   420,  4242, 12111,     \n",
    "                          0,     0,     0,     0,     0,     0,     0,\n",
    "                          0,     0,     0,     0,     0,     0,     0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Need 30 timesteps\n",
    "firstSentence = pad_sequences([firstSentence], maxlen = 30)[0]\n",
    "firstSentence = firstSentence.reshape((1, 30, 1))\n",
    "\n",
    "predict=model.predict(firstSentence, batch_size=1, verbose=1, steps=None)\n",
    "\n",
    "# make a prediction\n",
    "ynew = model.predict_classes(firstSentence)\n",
    "# show the inputs and predicted outputs\n",
    "for i in range(len(predict)):\n",
    "    print(\"X=%s, Predicted=%s\" % (predict[i], ynew[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict=model.predict(firstSentence, batch_size=1, verbose=1, steps=None)\n",
    "# make a prediction\n",
    "ynew = model.predict_classes(firstSentence)\n",
    "# show the inputs and predicted outputs\n",
    "for i in range(len(predict)):\n",
    "    print(\"X=%s, Predicted=%s\" % (predict[i], ynew[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
